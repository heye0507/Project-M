{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "from fastai.text import *\n",
    "from fastai import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('/home/jupyter/insight_project/Project-M/data/preprocessed/csv')\n",
    "filenames = path.ls()\n",
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "class Interface():\n",
    "    def __init__(self, path, bs=64, eval_mode=False):\n",
    "        '''\n",
    "        path must be a valid file path or file paths\n",
    "        path can be either str or Path obj\n",
    "        eval_mode set to False means create language model \n",
    "            and general classfier from scratch\n",
    "        '''\n",
    "        if not isinstance(path,(list,tuple)):\n",
    "            path = self._listify(path)     #if things are not iteratorable\n",
    "        self.path = self._clean_non_json_path(path) #remove non .json paths\n",
    "        self.csv_path = None\n",
    "        self.bs = bs\n",
    "        self.language_model_data = None\n",
    "        self.classification_model_data = None\n",
    "        self.eval_mode = eval_mode\n",
    "        self.df_train, self.df_valid = None, None\n",
    "        self.data_list, self.dataset_name = [], [] #to store all individual df for individual model\n",
    "        \n",
    "    def _clean_non_json_path(self,path):\n",
    "        filenames = []\n",
    "        for file_path in path:\n",
    "            file_extension = file_path.name.split('.')\n",
    "            if len(file_extension) < 2: continue\n",
    "            else:\n",
    "                if file_path.name.split('.')[1] == 'json':\n",
    "                    filenames.append(file_path)\n",
    "        return filenames\n",
    "    \n",
    "    def _listify(self,p):\n",
    "        '''Trun everying in p to a Path object'''\n",
    "        \n",
    "        if p is None: p = []\n",
    "        elif isinstance(p,str): p = [p]\n",
    "        else:\n",
    "            if not isinstance(p,Iterable): p = [p]\n",
    "        return [Path(i) for i in p]\n",
    "    \n",
    "    def _check_data_format(self):\n",
    "        if self.path == []: return False\n",
    "        one_file = self.path[0]\n",
    "        flag = 0\n",
    "        with open(one_file) as f:\n",
    "            data = json.load(f)\n",
    "        basilica_keys = ['data_points', 'num_data_points', 'id']\n",
    "        baslilca_data_keys = ['body', 'label', 'source', 'embedding']\n",
    "        for key in basilica_keys:\n",
    "            if not key in data.keys(): flag +=1\n",
    "        for key in baslilca_data_keys:\n",
    "            if not key in data['data_points'][0].keys(): flag +=1 #only check the first item...\n",
    "        if flag != 0:\n",
    "            print('Warning: data source not from Basilica, exit...')\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def _convert_json_to_csv(self,test=False):\n",
    "        #use self.path to grab json format Basilica data\n",
    "        #return csv file\n",
    "        #body,label,source\n",
    "        body, label, source = [], [], []\n",
    "        for one_file_path in self.path:\n",
    "            with open(one_file_path) as file:\n",
    "                data = json.load(file)\n",
    "            for item in data['data_points']:\n",
    "                body.append(item['body'])\n",
    "                label.append(item['label'])\n",
    "                source.append(item['source'])\n",
    "            df = pd.DataFrame(list(zip(body, label, source)),\n",
    "                             columns = ['Body', 'Label', 'Source'])\n",
    "        \n",
    "            #check if the parent directory exists\n",
    "            if test:\n",
    "                if not os.path.exists(one_file_path.parent/'delete_test'):\n",
    "                    os.makedirs(one_file_path.parent/'delete_test')\n",
    "            else:\n",
    "                if not os.path.exists(one_file_path.parent/'csv'):\n",
    "                    os.makedirs(one_file_path.parent/'csv')\n",
    "        \n",
    "            if test: dest = one_file_path.parent/'delete_test'\n",
    "            else: dest = one_file_path.parent/'csv'\n",
    "            filename = Path(one_file_path.name).stem + '.csv'\n",
    "            df.to_csv(dest/filename,index=False)\n",
    "            print(f'convert {filename} to csv')\n",
    "        self.csv_path = dest\n",
    "        \n",
    "    def _prepare_df(self):\n",
    "        dfs = []\n",
    "        for file in self.csv_path.ls():\n",
    "            df = pd.read_csv(file)\n",
    "            dfs.append(df)\n",
    "        return pd.concat(dfs)\n",
    "    \n",
    "    def _train_valid_split(self,total_filenames,train_pct=0.7):\n",
    "        '''split the train and test base on total datasets\n",
    "            return train_list and valid_list with their associated path'''\n",
    "        total_filenames = total_filenames.ls()\n",
    "        random.seed(42)\n",
    "        random.shuffle(total_filenames)\n",
    "\n",
    "        split_point = round(len(total_filenames)*train_pct)\n",
    "        assert (type(split_point) == int)\n",
    "        self.df_train, self.df_valid = total_filenames[:split_point],total_filenames[split_point:]\n",
    "        \n",
    "    def _get_lm(self,url):\n",
    "        '''This code needs to be updated once I figure out how to host pretrained weights\n",
    "            for now, it will use local path\n",
    "        '''\n",
    "        if url:\n",
    "            data_lm = load_data(url,'data_lm_large.pkl',bs=self.bs)\n",
    "        return data_lm\n",
    "        \n",
    "    def _get_individual_data(self,filepath,data_lm):\n",
    "        df = pd.read_csv(filepath)\n",
    "        if df.shape[0]*0.7 < self.bs: \n",
    "            bs = int(df.shape[0]*0.7)\n",
    "        else: bs=self.bs\n",
    "        data = (TextList\n",
    "                .from_df(df=df,path=self.csv_path,cols='Body',vocab=data_lm.vocab)\n",
    "                .split_by_rand_pct(0.3,seed=42)\n",
    "                .label_from_df(cols='Label')\n",
    "                .databunch(bs=bs,num_workers=os.cpu_count()*4)\n",
    "               )\n",
    "        return data\n",
    "        \n",
    "    \n",
    "    def pre_processing(self,skip_convert_json=False,test=False,**kwargs):\n",
    "        if not self._check_data_format(): exit(0) #we can't handle non Basilica data\n",
    "        if not skip_convert_json: self._convert_json_to_csv(test)\n",
    "        if self.eval_mode:\n",
    "            if 'lm_path' in kwargs.keys():\n",
    "                lm_path = kwargs['lm_path']\n",
    "            else:\n",
    "                print('Evaluation mode: lm_path not set...')\n",
    "                exit(0)\n",
    "            data_lm = self._get_lm(lm_path)\n",
    "            for file in self.csv_path.ls():\n",
    "                file_extension = Path(file).name.split('.')\n",
    "                if len(file_extension) < 2 or file_extension[1] != 'csv': continue\n",
    "                self.data_list.append(self._get_individual_data(file,data_lm))\n",
    "                self.dataset_name.append(file_extension[0])\n",
    "        else:\n",
    "            '''\n",
    "            Getting language model data bunch\n",
    "            Split general classifier train and valid\n",
    "            '''\n",
    "            df_total = self._prepare_df()\n",
    "            #Spacy Tokenization,pre_rules, post_rules applied,Numericalization\n",
    "            #create fastai databunch, which is a pair of train, valid dataloader of Pytorch\n",
    "            self.language_model_data = (TextList\n",
    "                                        .from_df(df_total,path=self.csv_path)\n",
    "                                        .split_by_rand_pct(0.1)\n",
    "                                        .label_for_lm()\n",
    "                                        .databunch(bs=self.bs,num_workers=16)\n",
    "                                        )\n",
    "            #Split data for general classifier\n",
    "            self._train_valid_split(self.csv_path)\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/ed91c398-31c6-437f-a9d1-462e3ccfb6fa.json'),\n",
       " PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/example.txt'),\n",
       " PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/ff063ea9-62b8-4f29-9faa-04a09cb5fba2.json'),\n",
       " PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/.ipynb_checkpoints'),\n",
       " PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/delete_test'),\n",
       " PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/fabab216-0767-4aa5-85fa-bb8852eb30d3.json'),\n",
       " PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/csv'),\n",
       " PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/f6d2081a-0f79-4abf-9021-c4d254859890.json')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('/home/jupyter/insight_project/Project-M/data/preprocessed/')\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Interface(path.ls(),eval_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/ed91c398-31c6-437f-a9d1-462e3ccfb6fa.json'),\n",
       " PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/ff063ea9-62b8-4f29-9faa-04a09cb5fba2.json'),\n",
       " PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/fabab216-0767-4aa5-85fa-bb8852eb30d3.json'),\n",
       " PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/f6d2081a-0f79-4abf-9021-c4d254859890.json')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.pre_processing(test=True,lm_path='/home/jupyter/insight_project/Project-M/data/preprocessed/csv/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test.data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ed91c398-31c6-437f-a9d1-462e3ccfb6fa',\n",
       " 'fabab216-0767-4aa5-85fa-bb8852eb30d3',\n",
       " 'ff063ea9-62b8-4f29-9faa-04a09cb5fba2',\n",
       " 'f6d2081a-0f79-4abf-9021-c4d254859890']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         if self.lm_retrain:\n",
    "#             df_total = _prepare_df()\n",
    "#             #Spacy Tokenization\n",
    "#             #pre_rules, post_rules applied\n",
    "#             #pre_rules: [fix_html, replace_rep, replace_wrep, spec_add_spaces, rm_useless_spaces]\n",
    "#             #post_rules: [replace_all_caps, deal_caps]\n",
    "#             #Numericalization\n",
    "#             #create fastai databunch, which is a pair of train, valid dataloader of Pytorch\n",
    "#             self.language_model_data = (TextList\n",
    "#                                         .from_df(df_total,path=path)\n",
    "#                                         .split_by_rand_pct(0.1)\n",
    "#                                         .label_for_lm()\n",
    "#                                         .databunch(bs=self.bs,num_workers=16)\n",
    "#                                         )\n",
    "#         else:\n",
    "#             #Have a language model already\n",
    "#             if self.language_model_path != None:\n",
    "#                 data_lm_path = Path(self.language_model_path).parent\n",
    "#             elif not os.path.exists(self.csv_path/'models'):\n",
    "#                 print('Warning: No language model found... aborting...')\n",
    "#                 exit(0)\n",
    "#             else: data_lm_path = self.csv_path/'models'\n",
    "#             self.language_model_data = load_data(data_lm_path, 'data_lm.pkl',bs=self.bs)\n",
    "#         self.cls_model_data = (TextList\n",
    "#                                .from_df()\n",
    "#                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook2script import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Test_module.ipynb to uti/interface.py\n"
     ]
    }
   ],
   "source": [
    "notebook2script('Test_module.ipynb','interface')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
