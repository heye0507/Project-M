{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "from fastai.text import *\n",
    "from fastai import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('/home/jupyter/insight_project/Project-M/data/preprocessed/csv')\n",
    "filenames = path.ls()\n",
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "class Interface():\n",
    "    def __init__(self, path, bs=64, eval_mode=False):\n",
    "        '''\n",
    "        path must be a valid file path or file paths\n",
    "        path can be either str or Path obj\n",
    "        eval_mode set to False means create language model \n",
    "            and general classfier from scratch\n",
    "        '''\n",
    "        if not isinstance(path,(list,tuple)):\n",
    "            path = self._listify(path)     #if things are not iteratorable\n",
    "        self.path = self._clean_non_json_path(path) #remove non .json paths\n",
    "        self.csv_path = None\n",
    "        self.bs = bs\n",
    "        self.language_model_data = None\n",
    "        self.classification_model_data = None\n",
    "        self.eval_mode = eval_mode\n",
    "        self.df_train, self.df_valid = None, None\n",
    "        \n",
    "    def _clean_non_json_path(self,path):\n",
    "        filenames = []\n",
    "        for file_path in path:\n",
    "            file_extension = file_path.name.split('.')\n",
    "            if len(file_extension) < 2: continue\n",
    "            else:\n",
    "                if file_path.name.split('.')[1] == 'json':\n",
    "                    filenames.append(file_path)\n",
    "        return filenames\n",
    "    \n",
    "    def _listify(self,p):\n",
    "        '''Trun everying in p to a Path object'''\n",
    "        \n",
    "        if p is None: p = []\n",
    "        elif isinstance(p,str): p = [p]\n",
    "        else:\n",
    "            if not isinstance(p,Iterable): p = [p]\n",
    "        return [Path(i) for i in p]\n",
    "    \n",
    "    def _check_data_format(self):\n",
    "        if self.path == []: return False\n",
    "        one_file = self.path[0]\n",
    "        flag = 0\n",
    "        with open(one_file) as f:\n",
    "            data = json.load(f)\n",
    "        basilica_keys = ['data_points', 'num_data_points', 'id']\n",
    "        baslilca_data_keys = ['body', 'label', 'source', 'embedding']\n",
    "        for key in basilica_keys:\n",
    "            if not key in data.keys(): flag +=1\n",
    "        for key in baslilca_data_keys:\n",
    "            if not key in data['data_points'][0].keys(): flag +=1 #only check the first item...\n",
    "        if flag != 0:\n",
    "            print('Warning: data source not from Basilica, exit...')\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def _convert_json_to_csv(self,test=False):\n",
    "        #use self.path to grab json format Basilica data\n",
    "        #return csv file\n",
    "        #body,label,source\n",
    "        body, label, source = [], [], []\n",
    "        for one_file_path in self.path:\n",
    "            with open(one_file_path) as file:\n",
    "                data = json.load(file)\n",
    "            for item in data['data_points']:\n",
    "                body.append(item['body'])\n",
    "                label.append(item['label'])\n",
    "                source.append(item['source'])\n",
    "            df = pd.DataFrame(list(zip(body, label, source)),\n",
    "                             columns = ['Body', 'Label', 'Source'])\n",
    "        \n",
    "            #check if the parent directory exists\n",
    "            if test:\n",
    "                if not os.path.exists(one_file_path.parent/'delete_test'):\n",
    "                    os.makedirs(one_file_path.parent/'delete_test')\n",
    "            else:\n",
    "                if not os.path.exists(one_file_path.parent/'csv'):\n",
    "                    os.makedirs(one_file_path.parent/'csv')\n",
    "        \n",
    "            if test: dest = one_file_path.parent/'delete_test'\n",
    "            else: dest = one_file_path.parent/'csv'\n",
    "            filename = Path(one_file_path.name).stem + '.csv'\n",
    "            df.to_csv(dest/filename,index=False)\n",
    "            print(f'convert {filename} to csv')\n",
    "        self.csv_path = dest\n",
    "        \n",
    "    def _prepare_df(self):\n",
    "        dfs = []\n",
    "        for file in self.csv_path.ls():\n",
    "            df = pd.read_csv(file)\n",
    "            dfs.append(df)\n",
    "        return pd.concat(dfs)\n",
    "    \n",
    "    def _train_valid_split(self,total_filenames,train_pct=0.7):\n",
    "        '''split the train and test base on total datasets\n",
    "            return train_list and valid_list with their associated path'''\n",
    "        total_filenames = total_filenames.ls()\n",
    "        random.seed(42)\n",
    "        random.shuffle(total_filenames)\n",
    "\n",
    "        split_point = round(len(total_filenames)*train_pct)\n",
    "        assert (type(split_point) == int)\n",
    "        self.df_train, self.df_valid = total_filenames[:split_point],total_filenames[split_point:]\n",
    "    \n",
    "    def pre_processing(self,skip_convert_json=False,test=False):\n",
    "        if not self._check_data_format(): exit(0) #we can't handle non Basilica data\n",
    "        if not skip_convert_json: self._convert_json_to_csv(test)\n",
    "        if self.eval_mode:\n",
    "            #TODO: set data_lm_path\n",
    "            #      load data_lm vocab\n",
    "            #      create individual classification data bunch using data_lm vocab\n",
    "            pass\n",
    "        else:\n",
    "            '''\n",
    "            Getting language model data bunch\n",
    "            Split general classifier train and valid\n",
    "            '''\n",
    "            df_total = self._prepare_df()\n",
    "            #Spacy Tokenization,pre_rules, post_rules applied,Numericalization\n",
    "            #create fastai databunch, which is a pair of train, valid dataloader of Pytorch\n",
    "            self.language_model_data = (TextList\n",
    "                                        .from_df(df_total,path=path)\n",
    "                                        .split_by_rand_pct(0.1)\n",
    "                                        .label_for_lm()\n",
    "                                        .databunch(bs=self.bs,num_workers=16)\n",
    "                                        )\n",
    "            #Split data for general classifier\n",
    "            self._train_valid_split(self.csv_path)\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/ed91c398-31c6-437f-a9d1-462e3ccfb6fa.json'),\n",
       " PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/example.txt'),\n",
       " PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/ff063ea9-62b8-4f29-9faa-04a09cb5fba2.json'),\n",
       " PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/delete_test'),\n",
       " PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/fabab216-0767-4aa5-85fa-bb8852eb30d3.json'),\n",
       " PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/csv'),\n",
       " PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/f6d2081a-0f79-4abf-9021-c4d254859890.json')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('/home/jupyter/insight_project/Project-M/data/preprocessed/')\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Interface(path.ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/ed91c398-31c6-437f-a9d1-462e3ccfb6fa.json'),\n",
       " PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/ff063ea9-62b8-4f29-9faa-04a09cb5fba2.json'),\n",
       " PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/fabab216-0767-4aa5-85fa-bb8852eb30d3.json'),\n",
       " PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/f6d2081a-0f79-4abf-9021-c4d254859890.json')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.pre_processing(test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/delete_test/ff063ea9-62b8-4f29-9faa-04a09cb5fba2.csv'),\n",
       " PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/delete_test/fabab216-0767-4aa5-85fa-bb8852eb30d3.csv'),\n",
       " PosixPath('/home/jupyter/insight_project/Project-M/data/preprocessed/delete_test/f6d2081a-0f79-4abf-9021-c4d254859890.csv')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextLMDataBunch;\n",
       "\n",
       "Train: LabelList (1329 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj oracle | xxmaj software xxmaj engineer | xxmaj oakland , xxup ca | xxup onsite | xxmaj full xxmaj time \n",
       " \n",
       "  xxmaj the xxmaj oracle xxmaj cloud xxmaj infrastructure , xxmaj registry team is looking for a senior xxmaj scala software engineer . xxmaj our service is architected as an xxmaj event - xxmaj sourced , xxmaj distributed system built on xxup cqrs principles . xxmaj our technology stack is xxmaj scala / xxmaj akka / xxmaj play . xxmaj if you have ever wondered what it is like to apply xxmaj functional xxmaj programming principles in a live , production environment this is the role for you . \n",
       " \n",
       "  xxmaj the xxmaj registry service is a managed , multi - tenant registry for xxmaj docker container images for customers who want to store and launch their containers on xxup oci , including xxmaj kubernetes users . xxmaj in addition to working on interesting projects and the future of xxup ocir , you 'll be getting a great salary , flexible working arrangements , a fun , state - of - the - art development environment and excellent opportunities for learning and career growth . xxmaj members of our team work on distributed systems in a cloud environment and have ownership of our service from top ( xxmaj load xxmaj balancers ) to bottom ( xxmaj databases ) . \n",
       " \n",
       "  xxmaj please contact justin.ko@oracle.com,xxbos xxmaj how # xxmaj kubernetes xxmaj changed xxmaj everything \n",
       " \n",
       "  xxmaj during xxup tc xxmaj sessions : xxmaj enterprise , three of the founding members of xxmaj kubernetes and the current director of product management for the project talk about the enterprise software development in the cloud using xxmaj kubernetes \n",
       " \n",
       "  https : / / t.co / r3ezmq1lk3 https : / / t.co / db0lksg5dh,xxbos xxmaj an overview of xxmaj tilt for teams managing local development instances of xxmaj kubernetes https : / / t.co / qysakd3xb5 https : / / t.co / 6wunhfxzby,xxbos xxmaj when xxmaj microsoft launched its xxmaj windows xxmaj subsystem for xxmaj linux ( xxup wsl ) back in 2018 , it was very clear why : xxmaj it wanted to provide tools for developers building modern cloud applications . xxmaj microsoft needed a way to offer container development , with an eye on xxmaj azure - hosted distributed applications running on its xxmaj azure xxmaj kubernetes xxmaj service . xxmaj to read this article in full , please click here(insider xxmaj story ),xxbos xxmaj when xxmaj microsoft launched its xxmaj windows xxmaj subsystem for xxmaj linux ( xxup wsl ) back in 2018 , it was very clear why : xxmaj it wanted to provide tools for developers building modern cloud applications . xxmaj microsoft needed a way to offer container development , with an eye on xxmaj azure - hosted distributed applications running on its xxmaj azure xxmaj kubernetes xxmaj service . xxmaj to read this article in full , please click here(insider xxmaj story )\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: /home/jupyter/insight_project/Project-M/data/preprocessed;\n",
       "\n",
       "Valid: LabelList (147 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj minikube based clusters , pushing pod level upgrades with helm , so you will need a helm chart for each specific pod . \n",
       " \n",
       "  xxmaj its simple as pie to mount local volumes into pods in kubernetes , you just need to have a values file in your chart for local dev that maps local code into the container . \n",
       " \n",
       "  xxmaj for iterative development i use a tool called draft.sh which is an awesome way to do cloud / local hybrid and allows you to use a non local k8s cluster as \" local \" which is nice .,xxbos xxmaj with growing threats , the need to tighten security adds a development challenge that can leave even fewer viable # microcontroller solutions . xxup @nxp ’s # armcortex - xxmaj m33-based # xxup mcu addresses this security need \n",
       "  xxmaj learn xxmaj more -- > https : / / t.co / maj0l85m7i https : / / t.co / payt90wtjs,xxbos @nitin_gadkari xxmaj the bill is passed but to translate this into action and outcomes a lot of change management has to take place in the rtos . xxmaj they are the ugliest offices of any state xxmaj govt and they must all be cleansed and modernised . xxmaj no bribes if no one goes to an xxup rto !,xxbos i 'm not saying it 's a good thing to fly in private jets , i 'm saying private jets contribute a xxunk amount of carbon to the atmosphere relative to other polluters . xxmaj one single corporation , xxmaj xxunk xxmaj xxunk xxmaj ships , puts out 10 times more xxunk dioxide than all of xxmaj europe 's cars combined . xxmaj my point is that these same corporations that put out the xxunk that you 're xxunk are the ones that contribute the most to pollution and climate change . xxmaj bernie 's carbon footprint is bigger than most xxmaj americans simply because he has to travel a lot . xxmaj he xxup is setting an example by xxunk for xxunk . \n",
       " \n",
       "  & ​ \n",
       " \n",
       "  xxmaj it frankly does n't matter how often xxmaj bernie flies in private planes or how many xxunk he xxunk . xxmaj he and most of the xxmaj democrats want to actually make laws to xxunk xxunk climate change . xxmaj individuals recycling or driving less on weekends or flying xxunk are n't going to do shit for the environment , unfortunately . xxmaj people should do those things , but regulation and laws are needed to really change things . \n",
       " \n",
       "  & ​ \n",
       " \n",
       "  i suspect you know all of this , though , and you 're just being xxunk for whatever reason .,xxbos xxmaj security researchers have discovered almost a dozen zero - day vulnerabilities in vxworks , one of the most widely used real - time operating systems ( xxup rtos ) for embedded devices that powers over 2 billion devices across aerospace , defense , industrial , medical , automotive , consumer electronics , networking , and other critical industries . xxmaj according to a new report xxmaj armis researchers shared with xxmaj the\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: /home/jupyter/insight_project/Project-M/data/preprocessed;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.language_model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         if self.lm_retrain:\n",
    "#             df_total = _prepare_df()\n",
    "#             #Spacy Tokenization\n",
    "#             #pre_rules, post_rules applied\n",
    "#             #pre_rules: [fix_html, replace_rep, replace_wrep, spec_add_spaces, rm_useless_spaces]\n",
    "#             #post_rules: [replace_all_caps, deal_caps]\n",
    "#             #Numericalization\n",
    "#             #create fastai databunch, which is a pair of train, valid dataloader of Pytorch\n",
    "#             self.language_model_data = (TextList\n",
    "#                                         .from_df(df_total,path=path)\n",
    "#                                         .split_by_rand_pct(0.1)\n",
    "#                                         .label_for_lm()\n",
    "#                                         .databunch(bs=self.bs,num_workers=16)\n",
    "#                                         )\n",
    "#         else:\n",
    "#             #Have a language model already\n",
    "#             if self.language_model_path != None:\n",
    "#                 data_lm_path = Path(self.language_model_path).parent\n",
    "#             elif not os.path.exists(self.csv_path/'models'):\n",
    "#                 print('Warning: No language model found... aborting...')\n",
    "#                 exit(0)\n",
    "#             else: data_lm_path = self.csv_path/'models'\n",
    "#             self.language_model_data = load_data(data_lm_path, 'data_lm.pkl',bs=self.bs)\n",
    "#         self.cls_model_data = (TextList\n",
    "#                                .from_df()\n",
    "#                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook2script import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Test_module.ipynb to uti/interface.py\n"
     ]
    }
   ],
   "source": [
    "notebook2script('Test_module.ipynb','interface')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
